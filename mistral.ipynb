{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets einops sentencepiece tokenizers\n",
    "! pip install git+https://github.com/huggingface/transformers\n",
    "! pip install torch\n",
    "! pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Open-Orca/Mistral-7B-OpenOrca\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Open-Orca/Mistral-7B-OpenOrca\").half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"A recommendation system for home, office and kitchen containers of all kinds. respond in bolivian spanish\" # set this to whatever you want like \"A recommendation system for server configuration\"\n",
    "prompt = \"I need to store dozens of folders and files in my office.\"\n",
    "\n",
    "prefix = \"<|im_start|>\"\n",
    "suffix = \"<|im_end|>\\n\"\n",
    "sys_format = prefix + \"system\\n\" + sys_prompt + suffix\n",
    "user_format = prefix + \"user\\n\" + prompt + suffix\n",
    "assistant_format = prefix + \"assistant\\n\"\n",
    "input_text = sys_format + user_format + assistant_format\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=1026, temperature=1.1, top_p=0.95, repetition_penalty=1.0,\n",
    "    do_sample=True, use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id,\n",
    "    transformers_version=\"4.34.0.dev0\")\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "# stop_words = set([\"the\", \"is\", \"in\", \"that\", \"does\", \"do\", \"and\", \"a\"]) # Total memory used: 1767.55 MB\n",
    "# stop_words = set([\"the\", \"is\", \"in\", \"that\", \"does\", \"do\"]) # Total memory used: 2445.98 MB\n",
    "stop_words = set([\"the\", \"is\", \"in\", \"that\", \"does\", \"do\", \"of\"]) # Sometimes removing stop words decreases memory usage\n",
    "\n",
    "\n",
    "def remove_stop_words(text, stop_words):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def get_gpu_memory():\n",
    "    torch.cuda.synchronize()\n",
    "    return torch.cuda.memory_allocated()\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "total_memory_used =  []\n",
    "\n",
    "# Function to run model inference\n",
    "def run_model(prompt):\n",
    "    # Tokenize input\n",
    "    \n",
    "    # Record initial memory usage\n",
    "    initial_memory = get_gpu_memory()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    \n",
    "    # Decode and print text\n",
    "    final_memory = get_gpu_memory()\n",
    "    memory_used = final_memory - initial_memory\n",
    "    total_memory_used.append(memory_used)\n",
    "    print(f\"Memory used by this task: {memory_used / (1024**2):.2f} MB\")\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    print(text)\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=1026, temperature=1.1, top_p=0.95, repetition_penalty=1.0,\n",
    "    do_sample=True, use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id,\n",
    "    transformers_version=\"4.34.0.dev0\")\n",
    "\n",
    "# Example prompts\n",
    "prompts = [\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nTell me about the euro-dollar\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nTell me about the TLT etf\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nWhat is deflation\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nwhat is inflation\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nHow does the fed work\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nwho is the fed chair\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nWhat is the GDP of Germany\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nwhat is the currency of Japan\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nWhat is a central bank\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nwho is the president of the ECB\",\n",
    "]\n",
    "prompts = [remove_stop_words(prompt, stop_words) for prompt in prompts]\n",
    "\n",
    "threads = []\n",
    "for prompt in prompts:\n",
    "    thread = threading.Thread(target=run_model, args=(prompt,))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "total_memory_used = sum(total_memory_used)\n",
    "print(f\"Total memory used: {total_memory_used / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_task_mem = 1292.56 / 6\n",
    "# Total memory used: 3594.73 MB with stopping words\n",
    "print(f\"Average memory used per task: {avg_task_mem:.2f} MB\")\n",
    "max_num_tasks = (25000 - 13500) / avg_task_mem\n",
    "print(f\"Max number of tasks: {max_num_tasks:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
