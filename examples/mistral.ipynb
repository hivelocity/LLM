{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets einops sentencepiece tokenizers\n",
    "! pip install git+https://github.com/huggingface/transformers\n",
    "! pip install torch\n",
    "! pip install protobuf\n",
    "! pip install beautifulsoup4\n",
    "! pip install zenpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(1)\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Open-Orca/Mistral-7B-OpenOrca\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Open-Orca/Mistral-7B-OpenOrca\").half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "url = \"https://core.hivelocity.net/api/v1/products/displays/www/fast\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data_json = json.loads(response.json())\n",
    "        all_locations = data_json['ALL_LOCATIONS']\n",
    "        path = os.getcwd() + \"/data.json\"\n",
    "        with open(path, 'w') as file:\n",
    "            json.dump(all_locations, file, indent=4)\n",
    "    pass\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "path = os.getcwd() + \"/data.json\"\n",
    "with open(path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "store_url = \"https://store.hivelocity.net/product/\"\n",
    "\n",
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    Cleans the input data by normalizing boolean values, removing HTML tags,\n",
    "    copying 'cores' from processor_text to display_text, and removing processor_text.\n",
    "\n",
    "    :param data: Dictionary containing the data to be cleaned.\n",
    "    :return: Cleaned data as a dictionary.\n",
    "    \"\"\"\n",
    "    def remove_html_tags(text):\n",
    "        return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    if 'processor_text' in data and 'display_text' in data:\n",
    "        if 'cores' in data['processor_text']:\n",
    "            cores_text = remove_html_tags(data['processor_text']['cores'])\n",
    "            data['display_text']['cores'] = cores_text.replace('(', '').replace(')', '')\n",
    "            \n",
    "        del data['processor_text']\n",
    "        data['description'] = data.pop('display_text')\n",
    "        \n",
    "    if 'id' in data:\n",
    "        data['product_id'] = data.pop('id')\n",
    "        data['product_store_url'] = f\"{store_url}{data['product_id']}/customize\"\n",
    "        \n",
    "    if 'category' in data and 'id' in data['category']:\n",
    "        data['category']['category_id'] = data['category'].pop('id')\n",
    "        \n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, bool):\n",
    "            data[key] = bool(value) \n",
    "        elif isinstance(value, dict):\n",
    "            data[key] = clean_data(value)\n",
    "            \n",
    "    if 'original_price' in data and 'price' in data and data['price'] < data['original_price']:\n",
    "        discount = data['original_price'] - data['price']\n",
    "        data['discount'] = {\n",
    "            'has_discount': True,\n",
    "            'discount_amount': discount,\n",
    "            'discount_percentage': round((discount / data['original_price']) * 100, 2)\n",
    "        }\n",
    "    return data\n",
    "\n",
    "product_json = [clean_data(item) for item in data if item['stock'] > 2]\n",
    "    \n",
    "len(product_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zenpy import Zenpy\n",
    "\n",
    "# Find real questions from clients and copy to prompt. (optinal)\n",
    "creds = {\n",
    "    'email': '',\n",
    "    'token': ''\n",
    "    'subdomain': 'hivelocity-hive'\n",
    "}\n",
    "zenpy_client = Zenpy(**creds)\n",
    "search_results = zenpy_client.search(type='ticket', query='sales question', per_page=5)\n",
    "\n",
    "for ticket in search_results:\n",
    "    first_question_found=False\n",
    "    if ticket.via.channel != 'chat':\n",
    "        for audit in zenpy_client.tickets.audits(ticket=ticket):\n",
    "            for event in audit.events:\n",
    "                if event['type'] == 'Comment':\n",
    "                    user = zenpy_client.users(id=event['author_id'])\n",
    "                    if user.role == 'end-user':\n",
    "                        print(f\"Ticket id: {ticket.id} - Comment: {event['body']} - Event id: {event['id']}\")\n",
    "                        first_question_found=True\n",
    "                        break\n",
    "            if first_question_found:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"Given the budget constraint, identify and recommend the top 3 servers\n",
    "              configurations available from Hivelocity Inc., a provider known for dedicated servers,\n",
    "              bare metal, colocation, and cloud hosting services. Focus on options that offer the best\n",
    "              value for money within this price range. Additionally, provide the store URL for each recommended\n",
    "              configuration. Note that the recommendations should be based on general market trends and \n",
    "              common offerings in the low-cost server segment as of early 2023. product must be in stock.\n",
    "              Advice if the product is smilar to the one requested but not exactly the same.\n",
    "              If product is not in found recommend something similar. Return product store url and product detail in a list. Use JSON Data for this task.\n",
    "              Add a reason why this servers works for your needs.\n",
    "              \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Prompt \n",
    "\n",
    "product_json = product_json[:25] # context only fits about 25 products\n",
    "prompt = \"I am looking for a server with 32GB of RAM, 2TB of storage, and 1Gbps of bandwidth.\"\n",
    "prefix = \"<|im_start|>\"\n",
    "suffix = \"<|im_end|>\\n\"\n",
    "sys_format = prefix + \"system\\n\" + sys_prompt + \"\\nJSON Data:\\n\" + json.dumps(product_json) + suffix\n",
    "user_format = prefix + \"user\\n\" + prompt + suffix\n",
    "assistant_format = prefix + \"assistant\\n\"\n",
    "input_text = sys_format + user_format + assistant_format\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=8000, temperature=0.6, top_p=0.95, repetition_penalty=1,\n",
    "    do_sample=True, use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id,\n",
    "    transformers_version=\"4.34.0.dev0\")\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple question and stop word prompt testing\n",
    "\n",
    "mport threading\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "# stop_words = set([\"the\", \"is\", \"in\", \"that\", \"does\", \"do\", \"and\", \"a\"]) # Total memory used: 1767.55 MB\n",
    "# stop_words = set([\"the\", \"is\", \"in\", \"that\", \"does\", \"do\"]) # Total memory used: 2445.98 MB\n",
    "stop_words = set([\"the\", \"is\", \"in\", \"that\", \"does\", \"do\", \"of\"]) # Sometimes removing stop words decreases memory usage\n",
    "\n",
    "\n",
    "def remove_stop_words(text, stop_words):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def get_gpu_memory():\n",
    "    torch.cuda.synchronize()\n",
    "    return torch.cuda.memory_allocated()\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "total_memory_used =  []\n",
    "\n",
    "# Function to run model inference\n",
    "def run_model(prompt):\n",
    "    # Tokenize input\n",
    "    \n",
    "    # Record initial memory usage\n",
    "    initial_memory = get_gpu_memory()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    \n",
    "    # Decode and print text\n",
    "    final_memory = get_gpu_memory()\n",
    "    memory_used = final_memory - initial_memory\n",
    "    total_memory_used.append(memory_used)\n",
    "    print(f\"Memory used by this task: {memory_used / (1024**2):.2f} MB\")\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    print(text)\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=1026, temperature=1.1, top_p=0.95, repetition_penalty=1.0,\n",
    "    do_sample=True, use_cache=True,\n",
    "    eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id,\n",
    "    transformers_version=\"4.34.0.dev0\")\n",
    "\n",
    "# Example prompts\n",
    "prompts = [\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nTell me about the euro-dollar\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nTell me about the TLT etf\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nWhat is deflation\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nwhat is inflation\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nHow does the fed work\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nwho is the fed chair\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nWhat is the GDP of Germany\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nwhat is the currency of Japan\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nWhat is a central bank\",\n",
    "    \"A system about economics and investment. Specialize in macro-economics and bonds.\\nwho is the president of the ECB\",\n",
    "]\n",
    "prompts = [remove_stop_words(prompt, stop_words) for prompt in prompts]\n",
    "\n",
    "threads = []\n",
    "for prompt in prompts:\n",
    "    thread = threading.Thread(target=run_model, args=(prompt,))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "total_memory_used = sum(total_memory_used)\n",
    "print(f\"Total memory used: {total_memory_used / (1024**2):.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
